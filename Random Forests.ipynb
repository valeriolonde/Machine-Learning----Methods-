{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "Is an ensemble method that combines multiple individual decision trees to make predictions. It improves upon the predictive accuracy of a single decision tree by averaging the predictions of each tree in the forest. In general, Random Forest tends to have superior performance compared to a single decision tree, and it often works well even with default parameters. However, as you continue to explore and build more models, you may come across other algorithms that offer even better performance. It's worth noting that some of these advanced models can be sensitive to finding the right combination of parameters for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# save filepath to variable for easier access\n",
    "melbourne_file_path = Path.cwd()\n",
    "\n",
    "# Define o caminho relativo para a pasta de dados\n",
    "data_folder = melbourne_file_path / \"melb_data.csv\"\n",
    "\n",
    "# read the data and store data in DataFrame titled melbourne_data\n",
    "melbourne_data = pd.read_csv(data_folder) \n",
    "\n",
    "# dropna drops missing values (think of na as \"not available\")\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "## Assigning the target variable 'Price' to y\n",
    "y = melbourne_data.Price\n",
    "\n",
    "## Defining a list of features to be used for modeling\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "\n",
    "## Creating a DataFrame X with only the selected features\n",
    "X = melbourne_data[melbourne_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207190.6873773146\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# Define the Random Forest model\n",
    "forest_model = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "forest_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "\n",
    "# Calculate the mean absolute error between the true and predicted values\n",
    "mae = mean_absolute_error(val_y, melb_preds)\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "There is likely room for further improvement, but this represents a significant improvement compared to the best decision tree error of 260,000 dolars. Random Forest models have parameters that can be tuned to enhance performance, similar to adjusting the maximum depth of a single decision tree. However, one of the key advantages of Random Forest models is that they often work reasonably well even without extensive tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
